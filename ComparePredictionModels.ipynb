{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ComparePredictionModels (1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "KL7ImG6hv0ER"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "from sklearn.metrics import make_scorer, accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import warnings\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.metrics import Recall\n",
        "from tensorflow.keras.metrics import Recall, FalseNegatives, Precision, AUC, Accuracy\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Data Loading**"
      ],
      "metadata": {
        "id": "pUj21k1bfUMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('ex4_data.csv')\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "Zqy0CsOGwJpD",
        "outputId": "1b2985a4-a05f-4ccb-b86e-724cbe7d5d97"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      INTENSIVE  NEWSITEID  RISK10YRS  INCLUSIONFRS  SBP  DBP  N_AGENTS  \\\n",
              "0       Regular       74.0  29.732061          True  145   80         2   \n",
              "1       Regular        8.0  29.677619          True  138   71         1   \n",
              "2     Intensive       25.0  17.443819          True  143   92         2   \n",
              "3       Regular       96.0   8.627849         False  123   68         2   \n",
              "4     Intensive       42.0  23.751437          True  126   51         2   \n",
              "...         ...        ...        ...           ...  ...  ...       ...   \n",
              "8741    Regular      102.0  10.896486         False  138   59         2   \n",
              "8742    Regular       98.0   8.646088         False  119   73         3   \n",
              "8743    Regular       99.0  24.191491          True  137   75         0   \n",
              "8744  Intensive       15.0  20.354619          True  154   93         0   \n",
              "8745  Intensive       74.0   7.459888         False  146   81         1   \n",
              "\n",
              "      NOAGENTS  SMOKE_3CAT  ASPIRIN  ...  RACE4  CHR  GLUR  HDL  TRR  UMALCR  \\\n",
              "0        False           3     True  ...  WHITE  155    81   36   92    5.80   \n",
              "1        False           2     True  ...  WHITE  243   107   61  188    5.45   \n",
              "2        False           2    False  ...  WHITE  180   116   47  125   13.33   \n",
              "3        False           1    False  ...  WHITE  234    93   89  109    6.12   \n",
              "4        False           2    False  ...  WHITE  126   108   39   84   28.78   \n",
              "...        ...         ...      ...  ...    ...  ...   ...  ...  ...     ...   \n",
              "8741     False           2     True  ...  WHITE  199    90   85   74   12.73   \n",
              "8742     False           2    False  ...  WHITE  145    83   52   95  440.38   \n",
              "8743      True           2    False  ...  WHITE  167    80   46   75   26.92   \n",
              "8744      True           2     True  ...  WHITE  245   103   37  369    3.20   \n",
              "8745     False           1     True  ...  WHITE  123    87   52   71    9.17   \n",
              "\n",
              "            BMI  STATIN  SBPTERTILE  EVENT_PRIMARY  \n",
              "0     33.115201    True           3          False  \n",
              "1     28.842380    True           2          False  \n",
              "2     33.643060   False           2          False  \n",
              "3     29.337871   False           1          False  \n",
              "4     36.660286   False           1          False  \n",
              "...         ...     ...         ...            ...  \n",
              "8741  27.186534    True           2          False  \n",
              "8742  42.191997    True           1          False  \n",
              "8743  19.462021    True           2          False  \n",
              "8744  35.579436   False           3          False  \n",
              "8745  30.783013    True           3          False  \n",
              "\n",
              "[8746 rows x 30 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-41ddb3a0-e386-4962-af08-c8439df76110\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>INTENSIVE</th>\n",
              "      <th>NEWSITEID</th>\n",
              "      <th>RISK10YRS</th>\n",
              "      <th>INCLUSIONFRS</th>\n",
              "      <th>SBP</th>\n",
              "      <th>DBP</th>\n",
              "      <th>N_AGENTS</th>\n",
              "      <th>NOAGENTS</th>\n",
              "      <th>SMOKE_3CAT</th>\n",
              "      <th>ASPIRIN</th>\n",
              "      <th>...</th>\n",
              "      <th>RACE4</th>\n",
              "      <th>CHR</th>\n",
              "      <th>GLUR</th>\n",
              "      <th>HDL</th>\n",
              "      <th>TRR</th>\n",
              "      <th>UMALCR</th>\n",
              "      <th>BMI</th>\n",
              "      <th>STATIN</th>\n",
              "      <th>SBPTERTILE</th>\n",
              "      <th>EVENT_PRIMARY</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Regular</td>\n",
              "      <td>74.0</td>\n",
              "      <td>29.732061</td>\n",
              "      <td>True</td>\n",
              "      <td>145</td>\n",
              "      <td>80</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>3</td>\n",
              "      <td>True</td>\n",
              "      <td>...</td>\n",
              "      <td>WHITE</td>\n",
              "      <td>155</td>\n",
              "      <td>81</td>\n",
              "      <td>36</td>\n",
              "      <td>92</td>\n",
              "      <td>5.80</td>\n",
              "      <td>33.115201</td>\n",
              "      <td>True</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Regular</td>\n",
              "      <td>8.0</td>\n",
              "      <td>29.677619</td>\n",
              "      <td>True</td>\n",
              "      <td>138</td>\n",
              "      <td>71</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "      <td>...</td>\n",
              "      <td>WHITE</td>\n",
              "      <td>243</td>\n",
              "      <td>107</td>\n",
              "      <td>61</td>\n",
              "      <td>188</td>\n",
              "      <td>5.45</td>\n",
              "      <td>28.842380</td>\n",
              "      <td>True</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Intensive</td>\n",
              "      <td>25.0</td>\n",
              "      <td>17.443819</td>\n",
              "      <td>True</td>\n",
              "      <td>143</td>\n",
              "      <td>92</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>WHITE</td>\n",
              "      <td>180</td>\n",
              "      <td>116</td>\n",
              "      <td>47</td>\n",
              "      <td>125</td>\n",
              "      <td>13.33</td>\n",
              "      <td>33.643060</td>\n",
              "      <td>False</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Regular</td>\n",
              "      <td>96.0</td>\n",
              "      <td>8.627849</td>\n",
              "      <td>False</td>\n",
              "      <td>123</td>\n",
              "      <td>68</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>WHITE</td>\n",
              "      <td>234</td>\n",
              "      <td>93</td>\n",
              "      <td>89</td>\n",
              "      <td>109</td>\n",
              "      <td>6.12</td>\n",
              "      <td>29.337871</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Intensive</td>\n",
              "      <td>42.0</td>\n",
              "      <td>23.751437</td>\n",
              "      <td>True</td>\n",
              "      <td>126</td>\n",
              "      <td>51</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>WHITE</td>\n",
              "      <td>126</td>\n",
              "      <td>108</td>\n",
              "      <td>39</td>\n",
              "      <td>84</td>\n",
              "      <td>28.78</td>\n",
              "      <td>36.660286</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8741</th>\n",
              "      <td>Regular</td>\n",
              "      <td>102.0</td>\n",
              "      <td>10.896486</td>\n",
              "      <td>False</td>\n",
              "      <td>138</td>\n",
              "      <td>59</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "      <td>...</td>\n",
              "      <td>WHITE</td>\n",
              "      <td>199</td>\n",
              "      <td>90</td>\n",
              "      <td>85</td>\n",
              "      <td>74</td>\n",
              "      <td>12.73</td>\n",
              "      <td>27.186534</td>\n",
              "      <td>True</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8742</th>\n",
              "      <td>Regular</td>\n",
              "      <td>98.0</td>\n",
              "      <td>8.646088</td>\n",
              "      <td>False</td>\n",
              "      <td>119</td>\n",
              "      <td>73</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>WHITE</td>\n",
              "      <td>145</td>\n",
              "      <td>83</td>\n",
              "      <td>52</td>\n",
              "      <td>95</td>\n",
              "      <td>440.38</td>\n",
              "      <td>42.191997</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8743</th>\n",
              "      <td>Regular</td>\n",
              "      <td>99.0</td>\n",
              "      <td>24.191491</td>\n",
              "      <td>True</td>\n",
              "      <td>137</td>\n",
              "      <td>75</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>WHITE</td>\n",
              "      <td>167</td>\n",
              "      <td>80</td>\n",
              "      <td>46</td>\n",
              "      <td>75</td>\n",
              "      <td>26.92</td>\n",
              "      <td>19.462021</td>\n",
              "      <td>True</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8744</th>\n",
              "      <td>Intensive</td>\n",
              "      <td>15.0</td>\n",
              "      <td>20.354619</td>\n",
              "      <td>True</td>\n",
              "      <td>154</td>\n",
              "      <td>93</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "      <td>...</td>\n",
              "      <td>WHITE</td>\n",
              "      <td>245</td>\n",
              "      <td>103</td>\n",
              "      <td>37</td>\n",
              "      <td>369</td>\n",
              "      <td>3.20</td>\n",
              "      <td>35.579436</td>\n",
              "      <td>False</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8745</th>\n",
              "      <td>Intensive</td>\n",
              "      <td>74.0</td>\n",
              "      <td>7.459888</td>\n",
              "      <td>False</td>\n",
              "      <td>146</td>\n",
              "      <td>81</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>...</td>\n",
              "      <td>WHITE</td>\n",
              "      <td>123</td>\n",
              "      <td>87</td>\n",
              "      <td>52</td>\n",
              "      <td>71</td>\n",
              "      <td>9.17</td>\n",
              "      <td>30.783013</td>\n",
              "      <td>True</td>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8746 rows × 30 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-41ddb3a0-e386-4962-af08-c8439df76110')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-41ddb3a0-e386-4962-af08-c8439df76110 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-41ddb3a0-e386-4962-af08-c8439df76110');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert boolean variables to integers"
      ],
      "metadata": {
        "id": "6m3HM4gPfrGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bool_cols=[\"INCLUSIONFRS\",\"NOAGENTS\",\"ASPIRIN\",\"SUB_CKD\",\"RACE_BLACK\",\"FEMALE\",\"SUB_CVD\",\"SUB_CLINICALCVD\",\"SUB_SUBCLINICALCVD\",\"SUB_SENIOR\",\"STATIN\",\"EVENT_PRIMARY\"]\n",
        "\n",
        "for i in bool_cols:\n",
        "  df[i]=df[i].astype(int)\n"
      ],
      "metadata": {
        "id": "M9wO0cF5wPgD"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Convert categorical variables to integers"
      ],
      "metadata": {
        "id": "gMcHs7aTgyX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat = pd.Categorical(df['INTENSIVE'])\n",
        "df['INTENSIVE']=cat\n",
        "df['INTENSIVE'] = df.INTENSIVE.cat.codes\n",
        "cat = pd.Categorical(df['RACE4'])\n",
        "df['RACE4']=cat\n",
        "df['RACE4'] = df.RACE4.cat.codes"
      ],
      "metadata": {
        "id": "tkDQ1CgcecXq"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replacing null values with the average of each column"
      ],
      "metadata": {
        "id": "KhPPGofEg4V4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.apply(lambda x: x.fillna(x.mean())) \n",
        "df"
      ],
      "metadata": {
        "id": "BziUq4RCfrAi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "559857d9-e14e-47a0-a566-530072314e83"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      INTENSIVE  NEWSITEID  RISK10YRS  INCLUSIONFRS  SBP  DBP  N_AGENTS  \\\n",
              "0             1       74.0  29.732061             1  145   80         2   \n",
              "1             1        8.0  29.677619             1  138   71         1   \n",
              "2             0       25.0  17.443819             1  143   92         2   \n",
              "3             1       96.0   8.627849             0  123   68         2   \n",
              "4             0       42.0  23.751437             1  126   51         2   \n",
              "...         ...        ...        ...           ...  ...  ...       ...   \n",
              "8741          1      102.0  10.896486             0  138   59         2   \n",
              "8742          1       98.0   8.646088             0  119   73         3   \n",
              "8743          1       99.0  24.191491             1  137   75         0   \n",
              "8744          0       15.0  20.354619             1  154   93         0   \n",
              "8745          0       74.0   7.459888             0  146   81         1   \n",
              "\n",
              "      NOAGENTS  SMOKE_3CAT  ASPIRIN  ...  RACE4  CHR  GLUR  HDL  TRR  UMALCR  \\\n",
              "0            0           3        1  ...      3  155    81   36   92    5.80   \n",
              "1            0           2        1  ...      3  243   107   61  188    5.45   \n",
              "2            0           2        0  ...      3  180   116   47  125   13.33   \n",
              "3            0           1        0  ...      3  234    93   89  109    6.12   \n",
              "4            0           2        0  ...      3  126   108   39   84   28.78   \n",
              "...        ...         ...      ...  ...    ...  ...   ...  ...  ...     ...   \n",
              "8741         0           2        1  ...      3  199    90   85   74   12.73   \n",
              "8742         0           2        0  ...      3  145    83   52   95  440.38   \n",
              "8743         1           2        0  ...      3  167    80   46   75   26.92   \n",
              "8744         1           2        1  ...      3  245   103   37  369    3.20   \n",
              "8745         0           1        1  ...      3  123    87   52   71    9.17   \n",
              "\n",
              "            BMI  STATIN  SBPTERTILE  EVENT_PRIMARY  \n",
              "0     33.115201       1           3              0  \n",
              "1     28.842380       1           2              0  \n",
              "2     33.643060       0           2              0  \n",
              "3     29.337871       0           1              0  \n",
              "4     36.660286       0           1              0  \n",
              "...         ...     ...         ...            ...  \n",
              "8741  27.186534       1           2              0  \n",
              "8742  42.191997       1           1              0  \n",
              "8743  19.462021       1           2              0  \n",
              "8744  35.579436       0           3              0  \n",
              "8745  30.783013       1           3              0  \n",
              "\n",
              "[8746 rows x 30 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3fa20fa3-cc71-4a31-821a-0b0de4440169\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>INTENSIVE</th>\n",
              "      <th>NEWSITEID</th>\n",
              "      <th>RISK10YRS</th>\n",
              "      <th>INCLUSIONFRS</th>\n",
              "      <th>SBP</th>\n",
              "      <th>DBP</th>\n",
              "      <th>N_AGENTS</th>\n",
              "      <th>NOAGENTS</th>\n",
              "      <th>SMOKE_3CAT</th>\n",
              "      <th>ASPIRIN</th>\n",
              "      <th>...</th>\n",
              "      <th>RACE4</th>\n",
              "      <th>CHR</th>\n",
              "      <th>GLUR</th>\n",
              "      <th>HDL</th>\n",
              "      <th>TRR</th>\n",
              "      <th>UMALCR</th>\n",
              "      <th>BMI</th>\n",
              "      <th>STATIN</th>\n",
              "      <th>SBPTERTILE</th>\n",
              "      <th>EVENT_PRIMARY</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>74.0</td>\n",
              "      <td>29.732061</td>\n",
              "      <td>1</td>\n",
              "      <td>145</td>\n",
              "      <td>80</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>155</td>\n",
              "      <td>81</td>\n",
              "      <td>36</td>\n",
              "      <td>92</td>\n",
              "      <td>5.80</td>\n",
              "      <td>33.115201</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>8.0</td>\n",
              "      <td>29.677619</td>\n",
              "      <td>1</td>\n",
              "      <td>138</td>\n",
              "      <td>71</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>243</td>\n",
              "      <td>107</td>\n",
              "      <td>61</td>\n",
              "      <td>188</td>\n",
              "      <td>5.45</td>\n",
              "      <td>28.842380</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>17.443819</td>\n",
              "      <td>1</td>\n",
              "      <td>143</td>\n",
              "      <td>92</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>180</td>\n",
              "      <td>116</td>\n",
              "      <td>47</td>\n",
              "      <td>125</td>\n",
              "      <td>13.33</td>\n",
              "      <td>33.643060</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>96.0</td>\n",
              "      <td>8.627849</td>\n",
              "      <td>0</td>\n",
              "      <td>123</td>\n",
              "      <td>68</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>234</td>\n",
              "      <td>93</td>\n",
              "      <td>89</td>\n",
              "      <td>109</td>\n",
              "      <td>6.12</td>\n",
              "      <td>29.337871</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>23.751437</td>\n",
              "      <td>1</td>\n",
              "      <td>126</td>\n",
              "      <td>51</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>126</td>\n",
              "      <td>108</td>\n",
              "      <td>39</td>\n",
              "      <td>84</td>\n",
              "      <td>28.78</td>\n",
              "      <td>36.660286</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8741</th>\n",
              "      <td>1</td>\n",
              "      <td>102.0</td>\n",
              "      <td>10.896486</td>\n",
              "      <td>0</td>\n",
              "      <td>138</td>\n",
              "      <td>59</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>199</td>\n",
              "      <td>90</td>\n",
              "      <td>85</td>\n",
              "      <td>74</td>\n",
              "      <td>12.73</td>\n",
              "      <td>27.186534</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8742</th>\n",
              "      <td>1</td>\n",
              "      <td>98.0</td>\n",
              "      <td>8.646088</td>\n",
              "      <td>0</td>\n",
              "      <td>119</td>\n",
              "      <td>73</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>145</td>\n",
              "      <td>83</td>\n",
              "      <td>52</td>\n",
              "      <td>95</td>\n",
              "      <td>440.38</td>\n",
              "      <td>42.191997</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8743</th>\n",
              "      <td>1</td>\n",
              "      <td>99.0</td>\n",
              "      <td>24.191491</td>\n",
              "      <td>1</td>\n",
              "      <td>137</td>\n",
              "      <td>75</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>167</td>\n",
              "      <td>80</td>\n",
              "      <td>46</td>\n",
              "      <td>75</td>\n",
              "      <td>26.92</td>\n",
              "      <td>19.462021</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8744</th>\n",
              "      <td>0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>20.354619</td>\n",
              "      <td>1</td>\n",
              "      <td>154</td>\n",
              "      <td>93</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>245</td>\n",
              "      <td>103</td>\n",
              "      <td>37</td>\n",
              "      <td>369</td>\n",
              "      <td>3.20</td>\n",
              "      <td>35.579436</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8745</th>\n",
              "      <td>0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>7.459888</td>\n",
              "      <td>0</td>\n",
              "      <td>146</td>\n",
              "      <td>81</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>123</td>\n",
              "      <td>87</td>\n",
              "      <td>52</td>\n",
              "      <td>71</td>\n",
              "      <td>9.17</td>\n",
              "      <td>30.783013</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8746 rows × 30 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3fa20fa3-cc71-4a31-821a-0b0de4440169')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3fa20fa3-cc71-4a31-821a-0b0de4440169 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3fa20fa3-cc71-4a31-821a-0b0de4440169');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Separate the column we want to predict"
      ],
      "metadata": {
        "id": "cOCLfXJhhWWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y = df['EVENT_PRIMARY']\n",
        "df=df.drop(columns=['EVENT_PRIMARY'])"
      ],
      "metadata": {
        "id": "DuBHnt0sg22U"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data normalization"
      ],
      "metadata": {
        "id": "UeIxifU6f9dm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "df = scaler.fit_transform(df)"
      ],
      "metadata": {
        "id": "mVx4mjswgAFD"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split to train and test"
      ],
      "metadata": {
        "id": "8JtIgbW9hy46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df, Y, test_size=1/3, random_state=1, stratify=Y)"
      ],
      "metadata": {
        "id": "64dEvPrihZl1"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Models**"
      ],
      "metadata": {
        "id": "MlIIHiJ9iFOF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear classifier model: Logistic Regression**\n",
        "\n",
        "This model uses a regression method to solve binary classification problems using a logistic function.\n",
        "\n",
        "We chose this model because it solves binary classification problems and our target column \"EVENT_PRIMARY\" is binary. In addition, in this dataset, there are many more cases of 'false' (0) than 'true'(1) in our target column. Therefore, we need to balance it. In Logistic Regression there is a 'class_weight' parameter that increases the 'class_weight' of class 0 relative to class 1."
      ],
      "metadata": {
        "id": "0q6LeWKKiSXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = LogisticRegression()\n",
        "\n",
        "parameters = {'class_weight': ['balanced'],\n",
        "              'max_iter':[100,500, 1000] ,\n",
        "              'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
        "              'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
        "              'C':[0.001,0.09,1,5,10]\n",
        "              }\n",
        "              \n",
        "# Type of scoring used to compare parameter combinations: Recall\n",
        "scorer = make_scorer(recall_score)\n",
        "\n",
        "# Run the grid search\n",
        "grid_obj = GridSearchCV(clf, parameters,scoring=scorer, return_train_score=True)\n",
        "grid_obj = grid_obj.fit(X_train, y_train)\n",
        "\n"
      ],
      "metadata": {
        "id": "7En6_yl4Prrj"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_obj.best_params_"
      ],
      "metadata": {
        "id": "bdl_8CVU2WsL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "355ecf89-4422-421c-d92a-22ddefef9e16"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'C': 0.001,\n",
              " 'class_weight': 'balanced',\n",
              " 'max_iter': 100,\n",
              " 'penalty': 'l2',\n",
              " 'solver': 'liblinear'}"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the clf to the best combination of parameters\n",
        "clf = grid_obj.best_estimator_\n",
        " \n",
        "clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "hZ0xtAMBPcWv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e16b3c8-0af0-4173-fc9f-50f834be4368"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=0.001, class_weight='balanced', solver='liblinear')"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scores(test, predict):\n",
        "  d = {}\n",
        "  d['Accuracy'] = accuracy_score(test, predict)\n",
        "  d['Recall'] = recall_score(test, predict)\n",
        "  d['Precision'] = precision_score(test, predict)\n",
        "  d['ROC AUC'] = roc_auc_score(test, predict)\n",
        "\n",
        "  return d"
      ],
      "metadata": {
        "id": "lYae7TzAwnWn"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LR_scores = scores(y_test, clf.predict(X_test))\n",
        "LR_scores_train = scores(y_train, clf.predict(X_train))"
      ],
      "metadata": {
        "id": "Y3RSEvn3xyZU"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Does the model overfit?"
      ],
      "metadata": {
        "id": "srlqpsugkSdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Testing Error: '+str(1-accuracy_score(y_test, clf.predict(X_test))))\n",
        "print('Train Error: '+str(1-accuracy_score(y_train, clf.predict(X_train))))\n",
        "\n",
        "print('Test Scores'+str(LR_scores))\n",
        "print('Train Scores'+str(LR_scores_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91AfZG6Ij-ED",
        "outputId": "ea42d073-ad9f-4807-ab9a-888bbaef607d"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Error: 0.36213991769547327\n",
            "Train Error: 0.36535162950257294\n",
            "Test Scores{'Accuracy': 0.6378600823045267, 'Recall': 0.6555555555555556, 'Precision': 0.10611510791366907, 'ROC AUC': 0.646125730994152}\n",
            "Train Scores{'Accuracy': 0.6346483704974271, 'Recall': 0.6852367688022284, 'Precision': 0.10870525850640743, 'ROC AUC': 0.6582827967571734}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overfitting may have occurred if the testing error is greater than the training error.\n",
        "\n",
        "In our model, the testing error and the training error are close.\n",
        "In addition, for each evaluation method, the training score is close to the test score. Therefore, the model doesn't overfit."
      ],
      "metadata": {
        "id": "jr6SaFHlkcXZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ensemble model: Random Forest Classifier**\n",
        "\n",
        "This model is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time.\n",
        "\n",
        "For classification tasks, the output of the random forest is the class selected by most trees.\n",
        "\n",
        "We chose this model because in this dataset there are many more cases of 'false' (0) than 'true'(1) in the target column, therefore we need to balance it. In the Random Forest model, there is a 'class_weight' parameter that increases the 'class_weight' of class 0 relative to class 1.\n"
      ],
      "metadata": {
        "id": "9NT0SThvjbtw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = RandomForestClassifier()\n",
        "\n",
        "# Choose some parameter combinations to try\n",
        "parameters = {'n_estimators': [4, 7, 9],\n",
        "              'max_features': ['log2', 'sqrt','auto'], \n",
        "              'criterion': ['entropy', 'gini'],\n",
        "              'max_depth': [2, 3, 5, 10], \n",
        "              'min_samples_split': [2, 3, 5],\n",
        "              'min_samples_leaf': [1,5,8],\n",
        "               'class_weight': ['balanced']}\n",
        "\n",
        "# Type of scoring used to compare parameter combinations: Recall\n",
        "scorer = make_scorer(recall_score)\n",
        "\n",
        "# Run the grid search\n",
        "grid_obj = GridSearchCV(clf, parameters,scoring=scorer, return_train_score=True)\n",
        "grid_obj = grid_obj.fit(X_train, y_train)\n",
        "print(grid_obj.best_params_)"
      ],
      "metadata": {
        "id": "y-nM6QpDdJwd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70f63472-3e51-46f2-9f96-1b1d50e627a9"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': 2, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'min_samples_split': 2, 'n_estimators': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the clf to the best combination of parameters\n",
        "clf = grid_obj.best_estimator_\n",
        "\n",
        "clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "RyZsRBVddj6H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "322a996f-a347-40b9-88ef-6ebe82a86c09"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(class_weight='balanced', criterion='entropy',\n",
              "                       max_depth=2, max_features='sqrt', min_samples_leaf=5,\n",
              "                       n_estimators=4)"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RF_scores = scores(y_test, clf.predict(X_test))\n",
        "RF_scores_train = scores(y_train, clf.predict(X_train))"
      ],
      "metadata": {
        "id": "ixPE9v1gz-nJ"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Does the model overfit?"
      ],
      "metadata": {
        "id": "vR-ckAYolEJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Test Error: '+str(1-accuracy_score(y_test, clf.predict(X_test))))\n",
        "print('Train Error: '+str(1-accuracy_score(y_train, clf.predict(X_train))))\n",
        "print('Test Scores'+str(RF_scores))\n",
        "print('Train Scores'+str(RF_scores_train))"
      ],
      "metadata": {
        "id": "jnCaFnEcBKtW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a55088dc-2ad5-4a5f-9e60-826616721077"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Error: 0.35288065843621397\n",
            "Train Error: 0.3423670668953688\n",
            "Test Scores{'Accuracy': 0.647119341563786, 'Recall': 0.5666666666666667, 'Precision': 0.09686609686609686, 'ROC AUC': 0.6095394736842105}\n",
            "Train Scores{'Accuracy': 0.6576329331046312, 'Recall': 0.5598885793871866, 'Precision': 0.09857773418342325, 'ROC AUC': 0.6119676857820597}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overfitting may have occurred if the testing error is greater than the training error.\n",
        "\n",
        "In our model, the testing error and the training error are close. In addition, for each evaluation method, the training score is close to the test score. Therefore, the model doesn't overfit."
      ],
      "metadata": {
        "id": "drySAAHjlKF_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deep Learning model: Sequential**\n",
        "\n",
        "This model is a fully connected neuron network that consists of at least three layers: an input layer, a hidden layer and an output layer. Except for the input nodes, each node is a neuron that uses a nonlinear activation function. MLP utilizes a supervised learning technique called backpropagation for training.\n",
        "\n",
        "We chose this model because it solves binary classification problems and our target column \"EVENT_PRIMARY\" is binary. In addition, in this model, we got better results than in other models.\n"
      ],
      "metadata": {
        "id": "9YIPx0ALjtSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train1 = to_categorical(y_train)\n",
        "y_test1 = to_categorical(y_test)"
      ],
      "metadata": {
        "id": "kbgLbOZRaCaz"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "# define the Sequential model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=29, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "# compile the Sequential model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', Recall(), Precision(), AUC()])\n",
        "\n",
        "# fit the Sequential model on the dataset\n",
        "history = model.fit(x=X_train, y=y_train1, batch_size=10, epochs=30, validation_split=1/3,  workers=-1)"
      ],
      "metadata": {
        "id": "4FRl9lj3tkUs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3bcba29-bcfc-4e12-8dc6-180f1453bcd7"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "389/389 [==============================] - 3s 4ms/step - loss: 0.3576 - accuracy: 0.9156 - recall: 0.9156 - precision: 0.9156 - auc: 0.9368 - val_loss: 0.2456 - val_accuracy: 0.9372 - val_recall: 0.9372 - val_precision: 0.9372 - val_auc: 0.9390\n",
            "Epoch 2/30\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.2542 - accuracy: 0.9390 - recall: 0.9390 - precision: 0.9390 - auc: 0.9432 - val_loss: 0.2318 - val_accuracy: 0.9372 - val_recall: 0.9372 - val_precision: 0.9372 - val_auc: 0.9476\n",
            "Epoch 3/30\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.2355 - accuracy: 0.9390 - recall: 0.9390 - precision: 0.9390 - auc: 0.9496 - val_loss: 0.2273 - val_accuracy: 0.9372 - val_recall: 0.9372 - val_precision: 0.9372 - val_auc: 0.9515\n",
            "Epoch 4/30\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.2273 - accuracy: 0.9390 - recall: 0.9390 - precision: 0.9390 - auc: 0.9537 - val_loss: 0.2244 - val_accuracy: 0.9372 - val_recall: 0.9372 - val_precision: 0.9372 - val_auc: 0.9553\n",
            "Epoch 5/30\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.2238 - accuracy: 0.9390 - recall: 0.9390 - precision: 0.9390 - auc: 0.9527 - val_loss: 0.2232 - val_accuracy: 0.9372 - val_recall: 0.9372 - val_precision: 0.9372 - val_auc: 0.9551\n",
            "Epoch 6/30\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.2222 - accuracy: 0.9390 - recall: 0.9390 - precision: 0.9390 - auc: 0.9550 - val_loss: 0.2227 - val_accuracy: 0.9372 - val_recall: 0.9372 - val_precision: 0.9372 - val_auc: 0.9574\n",
            "Epoch 7/30\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.2189 - accuracy: 0.9390 - recall: 0.9390 - precision: 0.9390 - auc: 0.9553 - val_loss: 0.2232 - val_accuracy: 0.9372 - val_recall: 0.9372 - val_precision: 0.9372 - val_auc: 0.9571\n",
            "Epoch 8/30\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.2147 - accuracy: 0.9390 - recall: 0.9390 - precision: 0.9390 - auc: 0.9586 - val_loss: 0.2226 - val_accuracy: 0.9372 - val_recall: 0.9372 - val_precision: 0.9372 - val_auc: 0.9582\n",
            "Epoch 9/30\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.2122 - accuracy: 0.9390 - recall: 0.9390 - precision: 0.9390 - auc: 0.9591 - val_loss: 0.2233 - val_accuracy: 0.9372 - val_recall: 0.9372 - val_precision: 0.9372 - val_auc: 0.9576\n",
            "Epoch 10/30\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.2137 - accuracy: 0.9390 - recall: 0.9390 - precision: 0.9390 - auc: 0.9588 - val_loss: 0.2230 - val_accuracy: 0.9372 - val_recall: 0.9372 - val_precision: 0.9372 - val_auc: 0.9579\n",
            "Epoch 11/30\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.2093 - accuracy: 0.9390 - recall: 0.9390 - precision: 0.9390 - auc: 0.9613 - val_loss: 0.2224 - val_accuracy: 0.9372 - val_recall: 0.9372 - val_precision: 0.9372 - val_auc: 0.9589\n",
            "Epoch 12/30\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.2080 - accuracy: 0.9390 - recall: 0.9390 - precision: 0.9390 - auc: 0.9631 - val_loss: 0.2229 - val_accuracy: 0.9372 - val_recall: 0.9372 - val_precision: 0.9372 - val_auc: 0.9567\n",
            "Epoch 13/30\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.2067 - accuracy: 0.9390 - recall: 0.9390 - precision: 0.9390 - auc: 0.9615 - val_loss: 0.2234 - val_accuracy: 0.9372 - val_recall: 0.9372 - val_precision: 0.9372 - val_auc: 0.9567\n",
            "Epoch 14/30\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.2077 - accuracy: 0.9390 - recall: 0.9390 - precision: 0.9390 - auc: 0.9622 - val_loss: 0.2237 - val_accuracy: 0.9372 - val_recall: 0.9372 - val_precision: 0.9372 - val_auc: 0.9551\n",
            "Epoch 15/30\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.2061 - accuracy: 0.9390 - recall: 0.9390 - precision: 0.9390 - auc: 0.9647 - val_loss: 0.2244 - val_accuracy: 0.9372 - val_recall: 0.9372 - val_precision: 0.9372 - val_auc: 0.9571\n",
            "Epoch 16/30\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.2044 - accuracy: 0.9390 - recall: 0.9390 - precision: 0.9390 - auc: 0.9643 - val_loss: 0.2243 - val_accuracy: 0.9372 - val_recall: 0.9372 - val_precision: 0.9372 - val_auc: 0.9564\n",
            "Epoch 17/30\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.2058 - accuracy: 0.9390 - recall: 0.9390 - precision: 0.9390 - auc: 0.9638 - val_loss: 0.2249 - val_accuracy: 0.9372 - val_recall: 0.9372 - val_precision: 0.9372 - val_auc: 0.9556\n",
            "Epoch 18/30\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.2048 - accuracy: 0.9390 - recall: 0.9390 - precision: 0.9390 - auc: 0.9638 - val_loss: 0.2244 - val_accuracy: 0.9372 - val_recall: 0.9372 - val_precision: 0.9372 - val_auc: 0.9554\n",
            "Epoch 19/30\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.2023 - accuracy: 0.9390 - recall: 0.9390 - precision: 0.9390 - auc: 0.9648 - val_loss: 0.2267 - val_accuracy: 0.9372 - val_recall: 0.9372 - val_precision: 0.9372 - val_auc: 0.9544\n",
            "Epoch 20/30\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.2029 - accuracy: 0.9390 - recall: 0.9390 - precision: 0.9390 - auc: 0.9642 - val_loss: 0.2247 - val_accuracy: 0.9372 - val_recall: 0.9372 - val_precision: 0.9372 - val_auc: 0.9559\n",
            "Epoch 21/30\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.2030 - accuracy: 0.9390 - recall: 0.9390 - precision: 0.9390 - auc: 0.9655 - val_loss: 0.2255 - val_accuracy: 0.9372 - val_recall: 0.9372 - val_precision: 0.9372 - val_auc: 0.9560\n",
            "Epoch 22/30\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.2004 - accuracy: 0.9390 - recall: 0.9390 - precision: 0.9390 - auc: 0.9670 - val_loss: 0.2256 - val_accuracy: 0.9372 - val_recall: 0.9372 - val_precision: 0.9372 - val_auc: 0.9552\n",
            "Epoch 23/30\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.1981 - accuracy: 0.9390 - recall: 0.9390 - precision: 0.9390 - auc: 0.9673 - val_loss: 0.2266 - val_accuracy: 0.9372 - val_recall: 0.9372 - val_precision: 0.9372 - val_auc: 0.9547\n",
            "Epoch 24/30\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.2005 - accuracy: 0.9390 - recall: 0.9390 - precision: 0.9390 - auc: 0.9664 - val_loss: 0.2274 - val_accuracy: 0.9372 - val_recall: 0.9372 - val_precision: 0.9372 - val_auc: 0.9531\n",
            "Epoch 25/30\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.1975 - accuracy: 0.9390 - recall: 0.9390 - precision: 0.9390 - auc: 0.9672 - val_loss: 0.2283 - val_accuracy: 0.9372 - val_recall: 0.9372 - val_precision: 0.9372 - val_auc: 0.9549\n",
            "Epoch 26/30\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.1943 - accuracy: 0.9390 - recall: 0.9390 - precision: 0.9390 - auc: 0.9691 - val_loss: 0.2288 - val_accuracy: 0.9372 - val_recall: 0.9372 - val_precision: 0.9372 - val_auc: 0.9519\n",
            "Epoch 27/30\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.1980 - accuracy: 0.9390 - recall: 0.9390 - precision: 0.9390 - auc: 0.9679 - val_loss: 0.2292 - val_accuracy: 0.9372 - val_recall: 0.9372 - val_precision: 0.9372 - val_auc: 0.9542\n",
            "Epoch 28/30\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.1970 - accuracy: 0.9390 - recall: 0.9390 - precision: 0.9390 - auc: 0.9680 - val_loss: 0.2285 - val_accuracy: 0.9372 - val_recall: 0.9372 - val_precision: 0.9372 - val_auc: 0.9520\n",
            "Epoch 29/30\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.1965 - accuracy: 0.9390 - recall: 0.9390 - precision: 0.9390 - auc: 0.9684 - val_loss: 0.2301 - val_accuracy: 0.9372 - val_recall: 0.9372 - val_precision: 0.9372 - val_auc: 0.9532\n",
            "Epoch 30/30\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.1952 - accuracy: 0.9390 - recall: 0.9390 - precision: 0.9390 - auc: 0.9676 - val_loss: 0.2282 - val_accuracy: 0.9372 - val_recall: 0.9372 - val_precision: 0.9372 - val_auc: 0.9537\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_scores = model.evaluate(X_test, y_test1, verbose=0)[1:]\n",
        "train_scores = model.evaluate(X_train, y_train1, verbose=0)[1:]"
      ],
      "metadata": {
        "id": "Yup4khmf0Rb6"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Does the model overfit?"
      ],
      "metadata": {
        "id": "dtod38r4lZnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Testing Error: '+str(1-test_scores[0]))\n",
        "print('Train Error: '+str(1-train_scores[0]))\n",
        "\n",
        "print('Test Scores'+str(test_scores))\n",
        "print('Train Scores'+str(train_scores))"
      ],
      "metadata": {
        "id": "dfOgI0Yilaf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f6848ef-000f-4c93-879e-160c401fb42a"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Error: 0.06172841787338257\n",
            "Train Error: 0.06157803535461426\n",
            "Test Scores[0.9382715821266174, 0.9382715821266174, 0.9382715821266174, 0.9535638093948364]\n",
            "Train Scores[0.9384219646453857, 0.9384219646453857, 0.9384219646453857, 0.9673780202865601]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overfitting may have occurred if the testing error is greater than the training error.\n",
        "\n",
        "In our model, the testing error and the training error are close. In addition, for each evaluation method the training score is close to the test score. Therefore, the model doesn't overfit."
      ],
      "metadata": {
        "id": "5g1ON6jplegy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In the DL model, what's the effect of batch size? In general, and do you see this effect in the current task?**\n",
        "\n",
        "Batch size controls the accuracy of the estimated error of the gradient while training neural networks. On one hand, a small batch size can converge faster than a large batch size, but a large batch size can reach optimum minima that a small batch size cannot reach. However, a large batch size will lead to poor generalization. Also, a small batch size can have a significant regularization effect because of its high variance, but it will require a small learning rate to prevent it from overshooting the minima.\n",
        "\n",
        "We saw this effect in this task when we were fine-tuning the model parameters- When the batch size was very large(10,000 samples) we got worse results. When the batch size was very small(1 sample) the runtime of all the epochs was very long, but we already got good results in the first epochs (rapid convergence). When the batch size was around 500 samples, we got good results with fast calculation time."
      ],
      "metadata": {
        "id": "MWg70TM6ltB5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **comparison between the models:**"
      ],
      "metadata": {
        "id": "pLy9K9HNlzn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({'Logistic Regression': LR_scores.values(),\n",
        "                   'Random Forest': RF_scores.values(),\n",
        "                   'Sequential': test_scores},\n",
        "                  index=LR_scores.keys())\n",
        "df.plot.bar(rot=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "jw-acIYE2q6o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "72329d7f-fe6b-498f-faaf-62ee71f8e3f4"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f6e71b239d0>"
            ]
          },
          "metadata": {},
          "execution_count": 127
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We received in the \"Sequential\" model the highest score in each of the evaluation methods.\n",
        "\n",
        "In the \"Logistic Regression\" and the \"Random Forest\" models:\n",
        " \n",
        "The \"Precision\" score is very low i.e small chance of being sick\\dead provided the prediction is positive.\n",
        "\n",
        "However, the \"Recall\" scores are larger i.e larger chance the prediction is positive provided being sick\\dead.\n",
        "\n",
        "The \"ROC AUC\" scores are larger i.e large \"Sensitivity\" and large Specificity (small (1-Specificity) ).\n",
        "\n",
        "\"Sensitivity\" - The test is positive provided the patient has a disease.\n",
        "\n",
        "\"Specificity\" - The test is negative provided the patient does not has a disease.\n",
        "\n",
        "Although the accuracy is high in all of the models, it is not a good enough measure for the above dataset because the data is unbalanced(more false than true in the target column). For example, if we determine that the prediction is always \"false\", we will usually be right, so the accuracy will be high, even though we missed important illness/death cases.\n",
        "\n",
        "In our opinion, the most important evaluation method for this dataset is \"Recall\" because we want to get positive predictions provided the patient has a disease. Therefore, for each model, we chose parameters that increase this score.\n"
      ],
      "metadata": {
        "id": "CEQ73hn5wbKP"
      }
    }
  ]
}